<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>0x7df</title><link href="http://0x7df.github.io/" rel="alternate"></link><link href="http://0x7df.github.io/feeds/computer-science.atom.xml" rel="self"></link><id>http://0x7df.github.io/</id><updated>2015-04-21T20:58:00+01:00</updated><entry><title>CUDA basics part 2</title><link href="http://0x7df.github.io/cuda-basics-part-2.html" rel="alternate"></link><updated>2015-04-21T20:58:00+01:00</updated><author><name>0x7df</name></author><id>tag:0x7df.github.io,2015-04-21:cuda-basics-part-2.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently, I posted &lt;a href="https://0x7df.wordpress.com/2015/04/05/cuda-basics-part-1/" title="CUDA basics part 1"&gt;a basic introduction to CUDA C for programming
GPUs&lt;/a&gt;,
which showed how to do a vector addition. This illustrated some of the
CUDA basic syntax, but it wasn't a complex- enough example to bring to
light some of the trickier issues to do with designing algorithms
carefully to minimise data movement. Here we move on to the more
complicated algorithm for matrix multiplication, &lt;em&gt;C = AB&lt;/em&gt;, where we'll
see that elements of the matrices get used multiple times, so we'll want
to put them in the shared memory to minimise the number of times they
get retrieved from the much slower global (or device) memory. We'll also
see that, because data that a thread puts into shared memory is only
accessible by the other threads in the same thread block, we need to be
careful how we do this.&lt;/p&gt;
&lt;h2&gt;Naive matrix multiplication in CUDA&lt;/h2&gt;
&lt;p&gt;First, let's ignore those concerns and put together the simplest
implementation of matrix multiplication; then we'll analyse the memory
access, and see how we can improve on it.&lt;/p&gt;
&lt;p&gt;Before we begin, however, some-error checking. Below is a function-like
C macro that will be used to surround each CUDA statement we execute
with a check of the return code. The return code is set to the
pre-defined variable &lt;code&gt;cudaSuccess&lt;/code&gt; if the statement executed
successfully, or an error value otherwise. (Hence, we declare the
variable that will contain the CUDA statement return to be type
&lt;code&gt;cudaError_t&lt;/code&gt;.) Where an error value is returned, we pass this to the
CUDA function &lt;code&gt;cudaGetErrorString&lt;/code&gt;, which returns an error message that
we can print.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;\#define cudaCheck(stmt)                                               
\\  
    do \\  
{                                                                \\  
        cudaError_t err = stmt;                                       
\\  
        if (err != cudaSuccess) \\  
{                          \\  
     printf("ERROR: failed to run %s\\n", stmt);                \\  
      printf("ERROR: CUDA error %s\\n", cudaGetErrorString(err)); \\  
      return -1;                                                 \\  
    }                                                              \\  
} while (0)&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Simple matrix multiplication kernel&lt;/h3&gt;
&lt;h3&gt;&lt;a href="https://0x7df.files.wordpress.com/2015/03/tiled_matrix_multiplication_1.png"&gt;&lt;img alt="tiled_matrix_multiplication_1" src="https://0x7df.files.wordpress.com/2015/03/tiled_matrix_multiplication_1.png?w=296" /&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Now for the kernel function. The way we've chosen to divide this problem
up amongst threads is to have each thread calculate a single element in
the output vector, &lt;em&gt;C&lt;/em&gt;. Mathematically, for an &lt;em&gt;m&lt;/em&gt;-by-&lt;em&gt;n&lt;/em&gt; matrix &lt;em&gt;A&lt;/em&gt; and
an &lt;em&gt;n&lt;/em&gt;-by-&lt;em&gt;p&lt;/em&gt; matrix &lt;em&gt;B&lt;/em&gt;, this is:&lt;/p&gt;
&lt;div class="math"&gt;$$ C_{i,j} = \sum_{k=1}^n A_{i,k}B_{k,j} $$&lt;/div&gt;
&lt;p&gt;for each of the &lt;em&gt;m&lt;/em&gt;-by-&lt;em&gt;p&lt;/em&gt; elements in &lt;em&gt;C&lt;/em&gt;. This is illustrated in the
figure, where the input matrices &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; are shown in grey, and the
result, matrix &lt;em&gt;C&lt;/em&gt;, in blue; a single element of &lt;em&gt;C&lt;/em&gt; is highlighted in
red, and the corresponding row and column of &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; are also
highlighted.&lt;/p&gt;
&lt;p&gt;We implement this in CUDA C as follows:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;__global__ void matrixMultiply(float *A, float *B,  
float *C, int numACols,  
int numBRows, int numBCols,  
    int numCRows, int numCCols)  
{  
        
// Get the row and column indices of the single  
// element of the output matrix that this thread  
// is dealing with  
    int col = threadIdx.x + blockDim.x*blockIdx.x;  
    int row = threadIdx.y + blockDim.y*blockIdx.y;  
      
// Calculate the output matrix element  
    if ((row &amp;lt; numCRows) &amp;amp;&amp;amp; (col &amp;lt; numCCols))  
{  
        float Ctmp = 0;  
        for (int k = 0; k &amp;lt; numACols; ++k)  
{  
            Ctmp += A[row*numACols+k]*B[k*numBCols+col];  
        }  
        C[row*numCCols + col] = Ctmp;  
    }  
}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This is reasonably simple. Each thread figures out which output matrix
element it is responsible for, simply by checking the thread indices. It
proceeds only if the element indices are within the correct bounds of
the output matrix, which may not be the case if there are more threads
than elements (because we have to have a whole number of thread blocks).
Where they are, it retrieves the correct row of &lt;em&gt;A&lt;/em&gt; and column of &lt;em&gt;B&lt;/em&gt;,
and calculates the corresponding single element of &lt;em&gt;C&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Naive matrix multiplication host code&lt;/h3&gt;
&lt;p&gt;For completeness, here is the host code. The new things here that we
didn't see in the vector multiplication example are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The use of the C macro &lt;code&gt;cudaCheck&lt;/code&gt; (defined above) for error
    checking&lt;/li&gt;
&lt;li&gt;The fact that the grid and the thread blocks are two-dimensional&lt;/li&gt;
&lt;li&gt;The call to &lt;code&gt;cudaDeviceSynchronize()&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;`&lt;/p&gt;
&lt;p&gt;int main(int argc, char **argv) {&lt;/p&gt;
&lt;p&gt;float &lt;em&gt;hostA, &lt;/em&gt;hostB, &lt;em&gt;hostC;&lt;br /&gt;
  float &lt;/em&gt;deviceA, &lt;em&gt;deviceB, &lt;/em&gt;deviceC;&lt;br /&gt;
  int numARows, numACols; // Rows, columns in the matrix A&lt;br /&gt;
  int numBRows, numBCols; // Rows, columns in the matrix B&lt;br /&gt;
  int numCRows, numCCols; // Rows, columns in the matrix C&lt;br /&gt;
    int sizeA, sizeB, sizeC; // Size in memory of each of A, B and C&lt;br /&gt;
    int gridXSize, gridYSize; // Number of thread blocks in x, y
dimensions of grid&lt;br /&gt;
    int blockSize; // Number of threads in block&lt;/p&gt;
&lt;p&gt;// Allocate and populate the A and B matrices&lt;br /&gt;
// hostA and hostB, and get numARows, numACols,&lt;br /&gt;
// numBRows, numBCols&lt;/p&gt;
&lt;p&gt;// Set numCRows and numCCols&lt;br /&gt;
    numCRows = numARows;&lt;br /&gt;
    numCCols = numBCols;&lt;br /&gt;
&lt;br /&gt;
  // Allocate the C matrix&lt;br /&gt;
    hostC = (float&lt;em&gt;)malloc(numCRows&lt;/em&gt;numCCols&lt;em&gt;sizeof(float));&lt;br /&gt;
&lt;br /&gt;
// Allocate GPU memory&lt;br /&gt;
sizeA = numARows&lt;/em&gt;numACols&lt;em&gt;sizeof(float);&lt;br /&gt;
sizeB = numBRows&lt;/em&gt;numBCols&lt;em&gt;sizeof(float);&lt;br /&gt;
sizeC = numCRows&lt;/em&gt;numCCols&lt;em&gt;sizeof(float);&lt;br /&gt;
cudaCheck(cudaMalloc((void &lt;strong&gt;) &amp;amp;deviceA, sizeA));&lt;br /&gt;
cudaCheck(cudaMalloc((void &lt;/strong&gt;) &amp;amp;deviceB, sizeB));&lt;br /&gt;
   cudaCheck(cudaMalloc((void &lt;/em&gt;*) &amp;amp;deviceC, sizeC));&lt;/p&gt;
&lt;p&gt;// Copy data to the GPU&lt;br /&gt;
   cudaCheck(cudaMemcpy(deviceA, hostA, sizeA,
cudaMemcpyHostToDevice));&lt;br /&gt;
   cudaCheck(cudaMemcpy(deviceB, hostB, sizeB,
cudaMemcpyHostToDevice));&lt;/p&gt;
&lt;p&gt;// Initialize the grid and block dimensions&lt;br /&gt;
   blockSize = 16;&lt;br /&gt;
   gridXSize = (numCCols-1)/blockSize + 1;&lt;br /&gt;
   gridYSize = (numCRows-1)/blockSize + 1;&lt;br /&gt;
   dim3 dimGrid(gridXSize, gridYSize, 1);&lt;br /&gt;
   dim3 dimBlock(blockSize, blockSize, 1);&lt;br /&gt;
&lt;br /&gt;
   // Launch the GPU Kernel&lt;br /&gt;
   matrixMultiply&amp;lt;&amp;lt;&lt;dimGrid,dimBlock&gt;&amp;gt;&amp;gt;(deviceA, deviceB,&lt;br /&gt;
deviceC, numACols,&lt;br /&gt;
numBRows, numBCols,&lt;br /&gt;
                               numCRows, numCCols);&lt;br /&gt;
  cudaDeviceSynchronize();&lt;/p&gt;
&lt;p&gt;// Copy the GPU memory back to the CPU&lt;br /&gt;
   cudaCheck(cudaMemcpy(hostC, deviceC, sizeC,
cudaMemcpyDeviceToHost));&lt;/p&gt;
&lt;p&gt;// Free the GPU memory&lt;br /&gt;
   cudaCheck(cudaFree(deviceA));&lt;br /&gt;
   cudaCheck(cudaFree(deviceB));&lt;br /&gt;
   cudaCheck(cudaFree(deviceC));&lt;br /&gt;
&lt;br /&gt;
  // Do something with the solution, free the host memory, return&lt;/p&gt;
&lt;p&gt;}&lt;br /&gt;
`&lt;/p&gt;
&lt;p&gt;The call to &lt;code&gt;cudaDeviceSynchronize()&lt;/code&gt; ensures that all threads have
finished before the host code proceeds any further.&lt;/p&gt;
&lt;h3&gt;Performance analysis of the naive implementation&lt;/h3&gt;
&lt;p&gt;Clearly, each of the &lt;em&gt;mp&lt;/em&gt; elements of &lt;em&gt;C&lt;/em&gt; requires a full row of &lt;em&gt;A&lt;/em&gt; and
a full column of &lt;em&gt;B&lt;/em&gt; - both of length &lt;em&gt;n&lt;/em&gt; - to be read from memory, and
one value to be written back. Hence there are &lt;em&gt;(2n + 1)mp&lt;/em&gt; memory
accesses. Re-examining the kernel, we see that there are two floating
point operations per iteration of the inner loop (one multiply and one
add), and &lt;em&gt;n&lt;/em&gt; iterations of that loop, which is completed for each of
the &lt;em&gt;mp&lt;/em&gt; elements in the product matrix. Hence, there are 2&lt;em&gt;nmp&lt;/em&gt; FLOP,
and the CGMA is 2&lt;em&gt;n&lt;/em&gt;/(2&lt;em&gt;n&lt;/em&gt; + 1); which is effectively 1, except when the
matrices are very small. With a memory bandwidth of 150 GB/s, the
algorithm is limited to just under 150/8 = 20 GFLOP/s (assuming double
precision), which is still less than 2% of the available compute of our
nominal 1 TFLOP GPU.&lt;/p&gt;
&lt;h2&gt;Improving on the naive implementation&lt;/h2&gt;
&lt;p&gt;However, it turns out that we can improve on this. So far, all the data
storage has been in global memory, because that's the only permissible
location for CUDA memory allocations in the host code, and that's where
the data stays unless we explicitly move it, once inside the kernel
function (we'll see how later). It's also clear that in this algorithm
data gets re-used frequently. Every row of matrix &lt;em&gt;A&lt;/em&gt; is used &lt;em&gt;p&lt;/em&gt; times
and every column of matrix &lt;em&gt;B&lt;/em&gt; is used &lt;em&gt;m&lt;/em&gt; times. If we contrive an
algorithm that gets the necessary data into shared memory before it is
needed, and keeps it there while it is being re-used, then we can
clearly reduce the global memory accesses.&lt;/p&gt;
&lt;p&gt;However, it's not as though we can read &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; into shared memory
and have them accessible to all the threads working on the computation;
shared memory isn't globally accessible, despite the name, but is
instead local to a single streaming multiprocessor, and only 'shared'
amongst the threads in whichever thread block is currently assigned to
the SM. Hence our goal is to ensure that the threads in a given thread
block have the subset of input data they need available in their SM's
shared memory, under the general assumption that because of the small
size of the shared memory, not all of the needed data will fit in at
once.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://0x7df.files.wordpress.com/2015/03/tiled_matrix_multiplication_2.png"&gt;&lt;img alt="tiled_matrix_multiplication_2" src="https://0x7df.files.wordpress.com/2015/03/tiled_matrix_multiplication_2.png?w=285" /&gt;&lt;/a&gt;Consider
a thread block covering an area of the product matrix &lt;em&gt;C&lt;/em&gt;, which is &lt;em&gt;a&lt;/em&gt;
rows high by &lt;em&gt;a&lt;/em&gt; columns wide, with the top-left element being &lt;em&gt;i&lt;/em&gt;,&lt;em&gt;j&lt;/em&gt;
and the bottom-right therefore being &lt;em&gt;i+a,j+a&lt;/em&gt;. This is shown in the
figure. To compute these values, the rows &lt;em&gt;i, i+1, ..., i+a&lt;/em&gt; of matrix
&lt;em&gt;A&lt;/em&gt; and columns &lt;em&gt;j, j+1, ..., j+a&lt;/em&gt; of matrix &lt;em&gt;B&lt;/em&gt; are required,
comprising horizontal and vertical strips, respectively, of dimension &lt;em&gt;a
× n&lt;/em&gt; elements. We assume in general these strips comprise too much data
to move all together to shared memory. Instead, we move a block of
elements from the strip of &lt;em&gt;A&lt;/em&gt;, and a block of elements from the strip
of &lt;em&gt;B&lt;/em&gt; - i.e. two blocks of size &lt;em&gt;a&lt;/em&gt; × &lt;em&gt;a&lt;/em&gt;, one from each matrix; we
will refer to these as &lt;em&gt;tiles&lt;/em&gt;. Performing matrix multiplication on
these two tiles creates a tile of partial sums in the &lt;em&gt;C&lt;/em&gt; elements. When
the next pair of tiles from &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; are retrieved, the partial sums
are further incremented, until eventually the full strips have been
processed and the final answers are available.&lt;/p&gt;
&lt;p&gt;There is still some duplication of global memory accesses, because any
given strip of &lt;em&gt;A&lt;/em&gt; will be required by all the thread blocks of the &lt;em&gt;C&lt;/em&gt;
matrix that share the same row indices; and any given strip of &lt;em&gt;B&lt;/em&gt; will
be required by all the thread blocks of the &lt;em&gt;C&lt;/em&gt; matrix that share the
same column indices. However, we can see that there is at least &lt;em&gt;some&lt;/em&gt;
re-use of data in shared memory; each sub-row of the tile from &lt;em&gt;A&lt;/em&gt; gets
re-used &lt;em&gt;a&lt;/em&gt; times (for the &lt;em&gt;a&lt;/em&gt; elements of the output matrix that have
the same row index), as does each sub-column of the tile from &lt;em&gt;B&lt;/em&gt;. This
data re-use reduces the retrievals from global memory by a factor of
&lt;em&gt;a&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Here is the kernel for tiled matrix multiplication.&lt;/p&gt;
&lt;p&gt;`&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;global&lt;/strong&gt; void matrixMultiply(float &lt;em&gt;A, float &lt;/em&gt;B, float *C,&lt;br /&gt;
int numARows, int numACols,&lt;br /&gt;
int numBRows, int numBCols,&lt;br /&gt;
int numCRows, int numCCols) {&lt;/p&gt;
&lt;p&gt;// Define device shared-memory storage for&lt;br /&gt;
// tiles of the matrices&lt;br /&gt;
// Scope: each tile is accessible by a single&lt;br /&gt;
// block of threads&lt;br /&gt;
&lt;strong&gt;shared&lt;/strong&gt; float tileA[TILE_WIDTH][TILE_WIDTH];&lt;br /&gt;
&lt;strong&gt;shared&lt;/strong&gt; float tileB[TILE_WIDTH][TILE_WIDTH];&lt;/p&gt;
&lt;p&gt;// Define abbreviated variables for the&lt;br /&gt;
// block and thread IDs&lt;br /&gt;
// Scope: stored in registers and therefore&lt;br /&gt;
// accessible by single threads&lt;br /&gt;
int bx =  blockIdx.x;&lt;br /&gt;
int by =  blockIdx.y;&lt;br /&gt;
int tx =  threadIdx.x;&lt;br /&gt;
int ty =  threadIdx.y;&lt;/p&gt;
&lt;p&gt;// Each thread is responsible for a single&lt;br /&gt;
// element of the product matrix C.&lt;br /&gt;
// Determine which element, from the block&lt;br /&gt;
// and thread indices&lt;br /&gt;
int row = by&lt;em&gt;TILE_WIDTH + ty;&lt;br /&gt;
int col = bx&lt;/em&gt;TILE_WIDTH + tx;&lt;/p&gt;
&lt;p&gt;// Initialise a temp variable for the solution&lt;br /&gt;
// for this matrix element&lt;br /&gt;
// Scope: in register, private to individual thread&lt;br /&gt;
float Ctemp = 0;&lt;/p&gt;
&lt;p&gt;// Loop over the tiles in the A and B matrices&lt;br /&gt;
// that will contribute to the calculation of&lt;br /&gt;
// this element in the product matrix. We are&lt;br /&gt;
// looping over columns of A for a given row&lt;br /&gt;
// (equal to the row index of the C element),&lt;br /&gt;
// and over rows of the B matrix for a given&lt;br /&gt;
// column index (equal to the column index of&lt;br /&gt;
// the C element)&lt;br /&gt;
int numTiles = (numACols-1)/TILE_WIDTH + 1;&lt;/p&gt;
&lt;p&gt;for (int tl = 0; tl &amp;lt; numTiles; ++tl) {&lt;/p&gt;
&lt;p&gt;// Load the tiles into shared memory, so all&lt;br /&gt;
// threads in the block have access to the&lt;br /&gt;
// whole tiles. Each thread needs to load only&lt;br /&gt;
// a single value of each of the A and B tiles.&lt;br /&gt;
if ((row &amp;lt; numARows) &amp;amp;&amp;amp; (tl&lt;em&gt;TILE_WIDTH + tx &amp;lt; numACols)) {&lt;br /&gt;
tileA[ty][tx] = A[row&lt;/em&gt;numACols + tl&lt;em&gt;TILE_WIDTH + tx];&lt;br /&gt;
} else {&lt;br /&gt;
tileA[ty][tx] = 0.;&lt;br /&gt;
}&lt;br /&gt;
if ((tl&lt;/em&gt;TILE_WIDTH + ty &amp;lt; numBRows) &amp;amp;&amp;amp; (col &amp;lt; numBCols)) {&lt;br /&gt;
tileB[ty][tx] = B[(tl&lt;em&gt;TILE_WIDTH + ty)&lt;/em&gt;numBCols + col];&lt;br /&gt;
} else {&lt;br /&gt;
tileB[ty][tx] = 0.;&lt;br /&gt;
}&lt;br /&gt;
__syncthreads();&lt;/p&gt;
&lt;p&gt;// Loop over the elements within the A and B&lt;br /&gt;
// tiles that contribute to this element of C&lt;br /&gt;
for (int k = 0; k &amp;lt; TILE_WIDTH; ++k) {&lt;br /&gt;
Ctemp += tileA[ty][k] * tileB[k][tx];&lt;br /&gt;
}&lt;br /&gt;
__syncthreads();&lt;br /&gt;
}&lt;/p&gt;
&lt;p&gt;// Write the final value into the output array&lt;br /&gt;
if ((row &amp;lt; numARows) &amp;amp;&amp;amp; (col &amp;lt; numBCols)) {&lt;br /&gt;
C[row*numBCols + col] = Ctemp;&lt;br /&gt;
}&lt;br /&gt;
}&lt;br /&gt;
`&lt;/p&gt;
&lt;p&gt;In each thread block, the &lt;em&gt;a&lt;/em&gt;^2^ threads load two float values each and
perform 2&lt;em&gt;a&lt;/em&gt; floating-point operations to compute the dot product of the
row and column sub-sections (both of length &lt;em&gt;a&lt;/em&gt;) required for the single
output matrix element it holds. Hence there are 2&lt;em&gt;a&lt;/em&gt; computations for
two memory loads, which gives a CGMA ratio of &lt;em&gt;a&lt;/em&gt;. For the naive
implementation it was 1, so we have improved the CGMA by a factor of &lt;em&gt;a&lt;/em&gt;
by tiling the data.&lt;/p&gt;
&lt;p&gt;There are a few other things to note in the kernel.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The use of the &lt;code&gt;__shared__&lt;/code&gt; identifier in the allocations statements
    for &lt;code&gt;tileA&lt;/code&gt; and &lt;code&gt;tileB&lt;/code&gt; (which are the temporary storage arrays for
    the tiles of &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt;). This keyword is how we cause the storage
    to be allocated in shared memory (and therefore it can be used only
    in &lt;code&gt;__device__&lt;/code&gt; functions, not &lt;code&gt;__host__&lt;/code&gt; functions).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TILE_WIDTH&lt;/code&gt; is a C macro that we assume has been defined elsewhere.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculation of the &lt;em&gt;C&lt;/em&gt; element indices &lt;code&gt;row&lt;/code&gt; and &lt;code&gt;col&lt;/code&gt; is done using
    &lt;code&gt;TILE_WIDTH&lt;/code&gt;, where previously &lt;code&gt;blockDim.x&lt;/code&gt; and &lt;code&gt;blockDim.y&lt;/code&gt;
    appeared. This works because we have &lt;em&gt;defined&lt;/em&gt; the tile to be the
    same size as the thread block. In theory it could be different, but
    doing so gives us the very convenient consequence that each thread
    needs only to load a single element from each of &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; into
    shared memory to construct the tiles. This means the host code that
    calls the kernel needs to use &lt;code&gt;TILE_WIDTH&lt;/code&gt; to define the block size:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;gridXSize = (numCCols-1)/TILE_WIDTH + 1;  
gridYSize = (numCRows-1)/TILE_WIDTH + 1;  
dim3 DimGrid(gridXSize, gridYSize, 1);  // gridSize blocks in the
grid  
dim3 DimBlock(TILE_WIDTH, TILE_WIDTH, 1); // blockSize threads in
each block  
matrixMultiply&amp;lt;&amp;lt;&amp;lt;DimGrid,DimBlock&amp;gt;&amp;gt;&amp;gt;(deviceA, deviceB,
deviceC, ...&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We have put some logic around the statements that transfer data to
    the shared-memory tile storage. Since we can't guarantee that there
    will be a whole number of thread blocks in the matrix, this prevents
    threads whose &lt;code&gt;row&lt;/code&gt;, &lt;code&gt;col&lt;/code&gt; indices are outside the bounds of either
    &lt;em&gt;A&lt;/em&gt; or &lt;em&gt;B&lt;/em&gt; from attempting to retrieve data that isn't there.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;The appearance of &lt;code&gt;__syncthreads()&lt;/code&gt;. This is a barrier
    synchronization across all threads that ensures all threads complete
    any work up to this point before any proceed further. Without this,
    some threads could move on to begin computing matrix elements before
    other threads have loaded the correct data into shared memory, and
    out-of-date data could be used.&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="c"></category><category term="cuda"></category><category term="gpu"></category><category term="hpc"></category><category term="massively parallel"></category><category term="parallel computing"></category><category term="parallel programming"></category></entry><entry><title>CUDA basics part 1</title><link href="http://0x7df.github.io/cuda-basics-part-1.html" rel="alternate"></link><updated>2015-04-05T20:59:00+01:00</updated><author><name>0x7df</name></author><id>tag:0x7df.github.io,2015-04-05:cuda-basics-part-1.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/CUDA"&gt;CUDA (Compute Unified Device
Architecture)&lt;/a&gt;is an extension of
&lt;a href="http://www.tutorialspoint.com/cprogramming/c_overview.htm"&gt;C/C++&lt;/a&gt;,
developed by &lt;a href="http://www.nvidia.com/page/home.html"&gt;NVIDIA&lt;/a&gt;, the
&lt;a href="http://www.webopedia.com/TERM/G/GPU.html"&gt;GPU&lt;/a&gt;manufacturer, for
programming their devices. (There is also a &lt;a href="https://www.pgroup.com/resources/cudafortran.htm"&gt;Fortran
version&lt;/a&gt;, developed by
&lt;a href="http://www.pgroup.com/"&gt;PGI&lt;/a&gt;.) The purpose of CUDA is to allow
developers to program GPUs much more easily than previously, and since
its inception in 2007, the use of GPUs has opened up beyond just
graphics to more general, e.g. scientific, computing, which is often
referred to as general-purpose GPU computing -
&lt;a href="http://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units"&gt;GPGPU&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;CUDA is proprietary, which in my opinion disqualifies it from use in
major code development. The lifetime of high-performance scientific and
engineering codes is typically decades, and given &lt;a href="http://dx.doi.org/10.1017/S0962492912000050"&gt;the uncertainty
surrounding supercomputing
architectures&lt;/a&gt;, a credible
candidate for a programming model needs to be supported by a wide range
of compilers and on a wide range of platforms. (A similar programming
language is &lt;a href="https://www.khronos.org/opencl/"&gt;OpenCL&lt;/a&gt;, which as the name
suggests, is an open standard, being developed by a consortium of
organisations.) However, the &lt;a href="https://www.coursera.org/course/hetero"&gt;point has been
made&lt;/a&gt;that CUDA is a useful
teaching vehicle for the basic concepts of programming heterogeneous,
many-core supercomputers.&lt;/p&gt;
&lt;h2&gt;Heterogeneous Computing&lt;/h2&gt;
&lt;p&gt;Let's assume here that the model is heterogeneous; i.e. there are CPUs
(hosts) and GPUs (devices) working in conjunction, and that the
application runs on the CPU host, handing specific, highly
&lt;a href="http://queue.acm.org/detail.cfm?id=1365499"&gt;data-parallel&lt;/a&gt; parts of the
program off to the device as and when appropriate. Also , we assume
initially that the CPU part is basically serial; that is, we're not
combining CUDA with &lt;a href="http://www.mpi-forum.org/"&gt;MPI&lt;/a&gt; at this stage.&lt;/p&gt;
&lt;p&gt;To exploit this kind of architecture, it's necessary to &lt;em&gt;kernelise&lt;/em&gt; the
code: identify parts of it suitable for a high level of concurrency,
turn them into kernel functions that are handed over to the GPU device.
These will typically be portions of the code that are highly
data-parallel - i.e. loops over large sets of data items where the
iterations of the loops are independent of each other. A nice example is
a simple "DAXPY" loop (i.e. a double precision &lt;em&gt;Ax&lt;/em&gt; + &lt;em&gt;y&lt;/em&gt; vector
addition) or "DAXBY" loop (&lt;em&gt;Ax&lt;/em&gt; × &lt;em&gt;y&lt;/em&gt;), implemented here in C:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;void vectorAdd(int n, double a, double *x, double *y) {
    for (int i = 0; i &amp;lt; n; ++i) {  
        y[i] = a*x[i] + y[i];
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that the element indices are all &lt;code&gt;i&lt;/code&gt;; there's no use of data from
previous iterations of the loop. It's this absence of &lt;em&gt;loop-carried
dependencies&lt;/em&gt; that makes this loop data-parallel, and therefore suitable
for threading. Essentially, we can calculate all the iterations of the
loop independently, in any order; hence we can pass it to a GPU and
invoke as many threads as there are elements, to do the work as
concurrently as possible.&lt;/p&gt;
&lt;h2&gt;GPU hardware overview&lt;/h2&gt;
&lt;h3&gt;Threading&lt;/h3&gt;
&lt;p&gt;This post is about how to program using CUDA, but to understand what's
going on it's necessary to have a minimum of knowledge about the
hardware architecture. A single GPU is comprised of a set of what are
known, in NVIDIA's terminology, as &lt;em&gt;streaming multiprocessors&lt;/em&gt; (SMs);
these are, according to &lt;a href="http://booksite.elsevier.com/9780123838728/"&gt;Hennessy and
Patterson&lt;/a&gt;, "multithreaded
&lt;a href="http://en.wikipedia.org/wiki/SIMD"&gt;SIMD&lt;/a&gt; processors", with the nearest
non-GPU equivalent being a multithreaded &lt;a href="http://www.phy.ornl.gov/csep/ca/node24.html"&gt;vector
processor&lt;/a&gt;. The typical
number of SMs in one GPU is between 2 and 30, varying from generation to
generation. Each SM can support a maximum number of threads at one time,
typically in the low thousands (e.g. 1,536) ; so overall the GPU can
handle tens of thousands of threads simultaneously. A key aspect of the
GPU is that, as well as being massively multithreaded, it also has a
SIMD aspect. The threads assigned to a streaming multiprocessor are
grouped into sets of 32 threads, called &lt;em&gt;warps&lt;/em&gt;. Each 32-thread warp is
dealt with by the SM in a SIMD fashion; that is, each instruction is
fetched once and executed for all 32 threads at the same time. So all
the threads in a particular warp are progressed in lock-step. Hence
there are two types of parallelism at play in a GPU:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Multithreading (a kind of&lt;a href="en.wikipedia.org/wiki/SPMD"&gt;SPMD - single program/multiple data
    parallelism&lt;/a&gt;), where different
    processors execute the same program independently, on different
    subsets of the data; and&lt;/li&gt;
&lt;li&gt;SIMD (single instruction/multiple data), where each processor is
    executing the same instruction at the same time as every other.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that the SIMD aspect causes problems in cases where, as a result of
the logic of the particular bit of code being executed, different
threads within the same warp end up going down different paths through
the code, and therefore require different instructions. The GPU can
handle this &lt;em&gt;control divergence&lt;/em&gt;, but execution becomes inefficient; so
it's something the programmer needs to be aware of and think explicitly
about avoiding.&lt;/p&gt;
&lt;h3&gt;Memory&lt;/h3&gt;
&lt;p&gt;As will become clear, it's also vitally important to understand the
memory hierarchy of a GPU. As well as, and separate from, the host CPU's
memory (and we'll ignore the
&lt;a href="http://www.bottomupcs.com/memory.html"&gt;hierarchy&lt;/a&gt;there), the GPU device
has several different levels of memory:&lt;/p&gt;
&lt;p&gt;&lt;img alt="gpu_layout" src="https://0x7df.files.wordpress.com/2015/02/gpu_layout.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The main &lt;em&gt;global memory&lt;/em&gt; or &lt;em&gt;device memory&lt;/em&gt;, which is accessible to
    all the threads on the GPU,&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;constant memory&lt;/em&gt;, also globally accessible,&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;shared memory&lt;/em&gt;, of which each streaming multiprocessor has its
    own private bank, accessible to only the threads on that SM, and&lt;/li&gt;
&lt;li&gt;The per-thread &lt;em&gt;private memory&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On reaching a portion of the code that is data parallel and suitable for
passing to the GPU, the programmer allocates memory in the device's
global memory, and then uses CUDA commands to transfer data items from
the CPU host memory into the global memory. Calculated quantities then
need to be explicitly transferred back again. This is where we see the
first key difference from &lt;a href="www.openmp.org"&gt;OpenMP&lt;/a&gt; threading; in that
model, variables and arrays in memory are declared as either private
(each thread has its own copy) or global (every thread sees the same bit
of memory); but either way they all reside on the same memory hardware.
In CUDA we explicitly have to transfer data from CPU to GPU memory space
and back again.&lt;/p&gt;
&lt;p&gt;The global (device) memory is the only memory that the host CPU can read
and write to. Transfers into shared memory and private memory can be
done only by the GPU itself.&lt;/p&gt;
&lt;h2&gt;Threads and blocks&lt;/h2&gt;
&lt;p&gt;When we implement a CUDA kernel function, which is a chunk of highly
data-parallel code that will be handled by a large set of threads
working concurrently, we arrange the threads into a &lt;em&gt;grid&lt;/em&gt; of &lt;em&gt;thread
blocks&lt;/em&gt;. We'll worry about why this is, later; for now just note that a
grid is a three-dimensional construction of &lt;em&gt;l ×&lt;/em&gt; &lt;em&gt;m&lt;/em&gt; × &lt;em&gt;n&lt;/em&gt; thread
blocks, each of which is three-dimensional grouping of &lt;em&gt;i&lt;/em&gt; × &lt;em&gt;j&lt;/em&gt; × &lt;em&gt;k&lt;/em&gt;
threads. The actual values of &lt;em&gt;i&lt;/em&gt;, &lt;em&gt;j&lt;/em&gt;, ... to be used are defined in
the host code by the special CUDA statements:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;dim3 gridDims(l,m,n);  
dim3 blockDims(i,j,l);  
myKernel&amp;lt;&amp;lt;&amp;lt;gridDims,blockDims&amp;gt;&amp;gt;&amp;gt;(args);
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The third statement launches the kernel function called &lt;code&gt;myKernel&lt;/code&gt;, and
is a standard C function call statement - i.e. &lt;code&gt;myKernel(args)&lt;/code&gt; - but
with the special CUDA notation - &lt;code&gt;&amp;lt;&amp;lt;&amp;lt;gridDims, blockDims&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; - rather
unpleasantly intruding between the function name and its arguments. The
previous two lines define the variables &lt;code&gt;gridDims&lt;/code&gt; and &lt;code&gt;blockDims&lt;/code&gt; as
having &lt;code&gt;dim3&lt;/code&gt; type.&lt;/p&gt;
&lt;p&gt;CUDA requires that the blocks be independent of each other; there is no
way to communicate between different blocks that are executing.&lt;/p&gt;
&lt;p&gt;The reason for grouping threads together into blocks, is so that the
streaming multiprocessor can switch between threads (or really between
warps, which are sets of 32 threads that are always executed in
lock-step) - this means that one warp is waiting for a memory access,
the SM can switch to another in the meantime.&lt;/p&gt;
&lt;h2&gt;Vector addition - host code&lt;/h2&gt;
&lt;p&gt;The host C code running on the CPU will look something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;int main(int argc, char **argv) {
    int n, gridSize;  
    float *hostX, *hostY;  
    float *deviceX, *deviceY;  
    cudaError_t err;

    // Allocate and populate the hostX and hostY vectors

    // Allocate GPU memory  
    int size = n*sizeof(float);  
    err = cudaMalloc((void **) &amp;amp;deviceX, size);  
    err = cudaMalloc((void **) &amp;amp;deviceY, size);

    // Copy memory to the GPU  
    err = cudaMemcpy(deviceX, hostX, size, cudaMemcpyHostToDevice);  
    err = cudaMemcpy(deviceY, hostY, size, cudaMemcpyHostToDevice);

    // Initialize the grid and block dimensions  
    dim3 gridDims(ceil(n/256),1,1);  
    dim3 blockDims(256,1,1);

    // Launch the GPU Kernel  
    myKernel&amp;lt;&amp;lt;&amp;lt;gridDims,blockDims&amp;gt;&amp;gt;&amp;gt;(n, deviceX, deviceY);

    // Copy the GPU memory back to the CPU  
    err = cudaMemcpy(hostY, deviceY, size, cudaMemcpyDeviceToHost);

    // Free the GPU memory  
    err = cudaFree(deviceX);  
    err = cudaFree(deviceY);

    // Do something with the solution, free the host  
    // arrays, return  
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the definition of the grid and block dimensions, we've chosen to have
256 threads per block, and therefore &lt;em&gt;n&lt;/em&gt;/256 blocks (and we've used the
ceiling function to make sure the number of blocks is rounded &lt;em&gt;up&lt;/em&gt; to
the nearest integer if &lt;em&gt;n&lt;/em&gt; isn't divisible by 256). The grid and the
blocks are one-dimensional, for simplicity.&lt;/p&gt;
&lt;p&gt;Hopefully, the various CUDA functions that are called - &lt;code&gt;cudaMalloc&lt;/code&gt;,
&lt;code&gt;cudaMemcpy&lt;/code&gt; and &lt;code&gt;cudaFree&lt;/code&gt; - are fairly self-explanatory, and are &lt;a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide"&gt;well
documented&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Vector addition - kernel code&lt;/h2&gt;
&lt;p&gt;The CUDA kernel function that is called looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;__global__ void vecAdd(int n, float *x, float *y) {  
    int i = threadIdx.x + blockDim.x*blockIdx.x;  
    if (i &amp;lt; n) y[i] = x[i] + y[i];  
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first difference from an ordinary C function is the &lt;code&gt;__global__&lt;/code&gt;
keyword at the beginning of the function declaration. The compiler needs
to distinguish between functions for the host CPU and kernel functions
intended for the GPU; it does this using the keywords &lt;code&gt;__host__&lt;/code&gt; for the
former, and either &lt;code&gt;__global__&lt;/code&gt; or &lt;code&gt;__device__&lt;/code&gt; for the latter. The
second difference is the existence of the pre-defined variables
&lt;code&gt;threadIdx&lt;/code&gt; and &lt;code&gt;blockIdx&lt;/code&gt;, which give the (&lt;em&gt;x&lt;/em&gt;, &lt;em&gt;y&lt;/em&gt;, &lt;em&gt;z&lt;/em&gt;) indices of
the thread within the block, and of the block within the grid,
respectively; and &lt;code&gt;blockDim&lt;/code&gt;, which gives the (&lt;em&gt;x&lt;/em&gt;, &lt;em&gt;y&lt;/em&gt;, &lt;em&gt;z&lt;/em&gt;) dimensions
of the block, as defined in the function call.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;if&lt;/code&gt; statement is included for the case where &lt;em&gt;n&lt;/em&gt; is not divisible
by 256 and therefore we have &lt;code&gt;ceil(n/256)&lt;/code&gt; blocks, resulting in there
being more threads than elements in the vector(s).&lt;/p&gt;
&lt;h2&gt;Parallel efficiency&lt;/h2&gt;
&lt;p&gt;If we ignore the multiplication by the constant &lt;em&gt;A&lt;/em&gt; for the moment, and
concentrate on the vector addition &lt;code&gt;y[i] = x[i] + y[i]&lt;/code&gt;, we can see that
there are three memory accesses (two reads and a write) for each
statement, and only one floating-point calculation (the addition). The
&lt;a href="http://www.greatlakesconsortium.org/events/GPUMulticore/Chapter4-CudaMemoryModel.pdf"&gt;compute-to-global-memory-access (CGMA)
ratio&lt;/a&gt;
is therefore 1:3, or 1/3. This is an important metric of the performance
of an application or section of code. We often refer to codes as being
either &lt;em&gt;compute-bound&lt;/em&gt; or &lt;em&gt;memory-bound&lt;/em&gt;, depending on whether the
limiting factor on improving their performance is the rate at which we
can do computations, or the rate at which we can retrieve and send data
to and from memory. We'll see now that for a typical GPU, this vector
addition operation is very clearly memory-limited.&lt;/p&gt;
&lt;p&gt;A typical memory bandwidth for a GPU might be, say, 200GB/s, which means
that we can load/store:&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{200\,\mathrm{GB/s}}{8\,\mathrm{B/memory\:access}} =
25 \times 10^9\,\mathrm{memory\:access/s} $$&lt;/div&gt;
&lt;p&gt;Crudely, this limits the actual computation rate to&lt;/p&gt;
&lt;div class="math"&gt;$$ 25 \times
10^9\,\mathrm{memory\:access/s}\,\times\,0.33\,\mathrm{FLOP}/\mathrm{memory\:access}
\approx 8\,\mathrm{GFLOP/s} $$&lt;/div&gt;
&lt;p&gt;The peak theoretical performance of the GPU might be, say, 1000 GFLOP/s
double-precision – i.e. the actual performance obtained is  less than 1%
of peak. This is the case no matter how many threads there are - the
limiting factor is how quickly data can be transferred between the
global memory and the processors.&lt;/p&gt;
&lt;p&gt;For this function - simple vector addition - there isn't a great deal we
can do about the fact that it's memory bound. You have to bring back
each pair of elements from &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; from memory, then put the result
back again - the number of memory operations is irreducible. For more
complex operations, where pieces of data are typically used multiple
times, the trick is to use shared memory, which is much, much faster
than global memory. However, there is obviously much less of it, so only
small chunks of data can be placed there at a time; this means that
programmers need to think very carefully about memory access patterns in
their code, to ensure that multiple uses of a given chunk of data are
grouped as closely together as possible in the flow of the program, so
data isn't continually being placed and replaced in shared memory, via
expensive global memory operations. This is analogous to moving data
from memory into local
&lt;a href="http://searchstorage.techtarget.com/definition/cache-memory"&gt;cache&lt;/a&gt; in
a normal CPU, except that in CUDA programming for GPUs, the programmer
is explicitly controlling movement of data between the shared and global
memory.&lt;/p&gt;
&lt;p&gt;Actually, one way to improve the vector addition might be to
utilise &lt;a href="www.cs.iastate.edu/~prabhu/Tutorial/PIPELINE/instrLevParal.html"&gt;instruction-level parallelism
(ILP)&lt;/a&gt;.
In the kernel, the single floating-point operation has to wait for both
of the input vector elements (&lt;code&gt;x[i]&lt;/code&gt; and &lt;code&gt;y[i]&lt;/code&gt;) to be retrieved from
global memory before it can begin. Hence if the global memory reads and
writes take &lt;em&gt;M&lt;/em&gt; clock cycles each, and the floating operation takes &lt;em&gt;N&lt;/em&gt;,
then the total number of clock cycles is &lt;em&gt;(M+1)+N+M&lt;/em&gt; (assuming the
second load begins one clock cycle after the first, but otherwise that
they are done simultaneously). This is &lt;em&gt;2M+N+1&lt;/em&gt;, so to go through it &lt;em&gt;k&lt;/em&gt;
times is &lt;em&gt;k(2M+N+1)&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;However, if within the kernel we loaded two values of each input vector,
say &lt;code&gt;x[i+1]&lt;/code&gt; and &lt;code&gt;y[i+1]&lt;/code&gt; as well as &lt;code&gt;x[i]&lt;/code&gt; and &lt;code&gt;y[i]&lt;/code&gt;, then the
computation of &lt;code&gt;x[i] + y[i]&lt;/code&gt; starts after &lt;em&gt;M+1&lt;/em&gt; cycles and takes &lt;em&gt;N&lt;/em&gt;
cycles, so the result is written back to global memory after &lt;em&gt;(M+1)+N+M&lt;/em&gt;
cycles as before; &lt;em&gt;but&lt;/em&gt;, x[i+1] can start to load after 2 cycles, and
y[i+1] after 3 cycles, so computation of &lt;code&gt;x[i+1] + y[i+1]&lt;/code&gt; can begin
after &lt;em&gt;M+3&lt;/em&gt; cycles, and still takes &lt;em&gt;N&lt;/em&gt;. Hence &lt;code&gt;y[i+1]&lt;/code&gt; has been written
back to global memory after &lt;em&gt;(M+3)+N+M&lt;/em&gt; cycles. This means the whole
operation to get &lt;code&gt;x[i] + y[i]&lt;/code&gt; and &lt;code&gt;x[i+1] + y[i+1]&lt;/code&gt; takes &lt;em&gt;(M+3)+N+M&lt;/em&gt;
cycles overall, which is only 2 cycles more than it took to get only
&lt;code&gt;x[i] + y[i]&lt;/code&gt; in the original kernel. For &lt;em&gt;k&lt;/em&gt; elements, it will take
&lt;em&gt;k(2M+N+3)/2&lt;/em&gt;. This is basically going to take half the time of the
original (as long as &lt;em&gt;2M+N&lt;/em&gt; is large enough that the constant doesn't
matter).&lt;/p&gt;
&lt;p&gt;Nothing has been done to reduce the
&lt;a href="http://www.hardwaresecrets.com/article/Understanding-RAM-Timings/26/2"&gt;latency&lt;/a&gt;of
the memory operations, or to do fewer of them; instead they've been
overlapped as much as possible. This is called &lt;em&gt;latency-hiding&lt;/em&gt; - doing
other useful things while waiting for data to return from memory.
Actually, at a different level, the concept of latency-hiding is also
fundamental to the GPU and threading; by dividing up the data to be
processed into small chunks, and having many different threads operate
on those chunks, the GPU has much more flexibility to schedule work so
that memory latency can be hidden. For this reason, GPUs are described
as &lt;em&gt;throughput-oriented&lt;/em&gt; - they are more concerned with operating on
lots of data concurrently so they need to worry less about latency;
whereas CPUs, on the other hand, are &lt;em&gt;latency-oriented&lt;/em&gt;, and are
designed with as many tricks as possible up their sleeve to reduce
latency, under the assumption of, basically, a sequential execution
model.&lt;/p&gt;
&lt;p&gt;If the same sequential processor were handling all the elements, like in
a typical CPU, then this
&lt;a href="cs.stanford.edu/people/eroberts/courses/soco/projects/risc/pipelining/"&gt;pipelining&lt;/a&gt;
is the sort of thing modern processors try to do for you anyway, without
you having to worry about it.&lt;/p&gt;
&lt;hr /&gt;
&lt;div style="background:#FFFFFF;margin:0 10px 10px 0;padding:0 10px 0 0;text-align:left;font-family:Arial, Helvetica, sans-serif;line-height:1em;"&gt;
&lt;div style="font-size:11px;padding:0 0 10px;font-weight:bold;color:#045989;"&gt;

High-performance computing systems: Status and outlook

&lt;/div&gt;

&lt;div style="font-size:11px;"&gt;

J. J. Dongarra and A. J. van der Steen (2012).
&lt;a href="http://journals.cambridge.org/action/displayJournal?jid=ANU"&gt;Acta Numerica&lt;/a&gt;,
&lt;a href="http://journals.cambridge.org/action/displayIssue?iid=8539365"&gt;Volume 21 &lt;/a&gt;,
&lt;a href="http://journals.cambridge.org/action/displayAbstract?aid=8539374"&gt;May 2012, pp.379-474&lt;/a&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="c"></category><category term="cuda"></category><category term="gpu"></category><category term="hpc"></category><category term="massively parallel"></category><category term="parallel computing"></category><category term="parallel programming"></category></entry></feed>