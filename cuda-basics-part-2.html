<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Introduction Recently, I posted a basic introduction to CUDA C for programming GPUs, which showed how to do a vector addition. This illustrated some of the CUDA basic syntax, but it wasn&#39;t a...">
        <meta name="keywords" content="c, cuda, gpu, hpc, massively parallel, parallel computing, parallel programming">
        <link rel="icon" href="https://0x7df.github.io/favicon.ico">

        <title>CUDA basics part 2 - 0x7df</title>

        <!-- Stylesheets -->
        <link href="https://0x7df.github.io/theme/css/all.min.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <link href="https://0x7df.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="0x7df Full Atom Feed" />
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->



    </head>

    <body>

        <!-- Header -->
    <div class="header-container gradient">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="https://0x7df.github.io/"><img class="mr20" src="https://0x7df.github.io/images/0x7df_60x60.png" alt="logo">0x7df</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="https://0x7df.github.io/">Blog Home</a>
                                <a href="http://0x7df.io">Site Home</a>
                                <a href="https://0x7df.github.io/categories.html">Categories</a>
                            <a  href="https://0x7df.github.io/pages/0x7df-application.html">0x7df application</a>
                            <a  href="https://0x7df.github.io/pages/about.html">About</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title">CUDA basics part 2</h1>
                      <p class="header-date">By <a href="https://0x7df.github.io/author/0x7df.html">0x7df</a>, Tue 21 April 2015, in category <a href="https://0x7df.github.io/category/computer-science.html">Computer science</a></p>
                      <div class="header-underline"></div>
                      <div class="clearfix"></div>
                      <p class="pull-right header-tags">
                          <span class="glyphicon glyphicon-tags mr5" aria-hidden="true"></span>
<a href="https://0x7df.github.io/tag/c.html">c</a>, <a href="https://0x7df.github.io/tag/cuda.html">cuda</a>, <a href="https://0x7df.github.io/tag/gpu.html">gpu</a>, <a href="https://0x7df.github.io/tag/hpc.html">hpc</a>, <a href="https://0x7df.github.io/tag/massively-parallel.html">massively parallel</a>, <a href="https://0x7df.github.io/tag/parallel-computing.html">parallel computing</a>, <a href="https://0x7df.github.io/tag/parallel-programming.html">parallel programming</a>                      </p>
                  </div>
              </div>
        </div>
    </div>
    <!-- /Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="container content">
        <h2>Introduction</h2>
<p>Recently, I posted <a href="https://0x7df.github.io/cuda-basics-part-1.html" title="CUDA basics part 1">a basic introduction to CUDA C for programming
GPUs</a>,
which showed how to do a vector addition. This illustrated some of the
CUDA basic syntax, but it wasn't a complex- enough example to bring to
light some of the trickier issues to do with designing algorithms
carefully to minimise data movement. Here we move on to the more
complicated algorithm for matrix multiplication, <em>C = AB</em>, where we'll
see that elements of the matrices get used multiple times, so we'll want
to put them in the shared memory to minimise the number of times they
get retrieved from the much slower global (or device) memory. We'll also
see that, because data that a thread puts into shared memory is only
accessible by the other threads in the same thread block, we need to be
careful how we do this.</p>
<h2>Naive matrix multiplication in CUDA</h2>
<p>First, let's ignore those concerns and put together the simplest
implementation of matrix multiplication; then we'll analyse the memory
access, and see how we can improve on it.</p>
<p>Before we begin, however, some-error checking. Below is a function-like
C macro that will be used to surround each CUDA statement we execute
with a check of the return code. The return code is set to the
pre-defined variable <code>cudaSuccess</code> if the statement executed
successfully, or an error value otherwise. (Hence, we declare the
variable that will contain the CUDA statement return to be type
<code>cudaError_t</code>.) Where an error value is returned, we pass this to the
CUDA function <code>cudaGetErrorString</code>, which returns an error message that
we can print.</p>
<p><code>\#define cudaCheck(stmt)                                               
\\  
    do \\  
{                                                                \\  
        cudaError_t err = stmt;                                       
\\  
        if (err != cudaSuccess) \\  
{                          \\  
     printf("ERROR: failed to run %s\\n", stmt);                \\  
      printf("ERROR: CUDA error %s\\n", cudaGetErrorString(err)); \\  
      return -1;                                                 \\  
    }                                                              \\  
} while (0)</code></p>
<h3>Simple matrix multiplication kernel</h3>
<p><img alt="tiled_matrix_multiplication_1" src="https://0x7df.github.io/images/tiled_matrix_multiplication_1.png?w=296"></p>
<p>Now for the kernel function. The way we've chosen to divide this problem
up amongst threads is to have each thread calculate a single element in
the output vector, <em>C</em>. Mathematically, for an <em>m</em>-by-<em>n</em> matrix <em>A</em> and
an <em>n</em>-by-<em>p</em> matrix <em>B</em>, this is:</p>
<div class="math">$$ C_{i,j} = \sum_{k=1}^n A_{i,k}B_{k,j} $$</div>
<p>for each of the <em>m</em>-by-<em>p</em> elements in <em>C</em>. This is illustrated in the
figure, where the input matrices <em>A</em> and <em>B</em> are shown in grey, and the
result, matrix <em>C</em>, in blue; a single element of <em>C</em> is highlighted in
red, and the corresponding row and column of <em>A</em> and <em>B</em> are also
highlighted.</p>
<p>We implement this in CUDA C as follows:</p>
<div class="highlight"><pre><span></span><code><span class="n">__global__</span> <span class="kt">void</span> <span class="n">matrixMultiply</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span>  
<span class="kt">float</span> <span class="o">*</span><span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numACols</span><span class="p">,</span>  
<span class="kt">int</span> <span class="n">numBRows</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numBCols</span><span class="p">,</span>  
    <span class="kt">int</span> <span class="n">numCRows</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numCCols</span><span class="p">)</span>  
<span class="p">{</span>  
        
<span class="c1">// Get the row and column indices of the single  </span>
<span class="c1">// element of the output matrix that this thread  </span>
<span class="c1">// is dealing with  </span>
    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>  
    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>  
      
<span class="c1">// Calculate the output matrix element  </span>
    <span class="k">if</span> <span class="p">((</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">numCRows</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">numCCols</span><span class="p">))</span>  
<span class="p">{</span>  
        <span class="kt">float</span> <span class="n">Ctmp</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>  
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">numACols</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span>  
<span class="p">{</span>  
            <span class="n">Ctmp</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">numACols</span><span class="o">+</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="o">*</span><span class="n">numBCols</span><span class="o">+</span><span class="n">col</span><span class="p">];</span>  
        <span class="p">}</span>  
        <span class="n">C</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">numCCols</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">Ctmp</span><span class="p">;</span>  
    <span class="p">}</span>  
<span class="p">}</span>
</code></pre></div>

<p>This is reasonably simple. Each thread figures out which output matrix
element it is responsible for, simply by checking the thread indices. It
proceeds only if the element indices are within the correct bounds of
the output matrix, which may not be the case if there are more threads
than elements (because we have to have a whole number of thread blocks).
Where they are, it retrieves the correct row of <em>A</em> and column of <em>B</em>,
and calculates the corresponding single element of <em>C</em>.</p>
<h3>Naive matrix multiplication host code</h3>
<p>For completeness, here is the host code. The new things here that we
didn't see in the vector multiplication example are:</p>
<ol>
<li>The use of the C macro <code>cudaCheck</code> (defined above) for error checking</li>
<li>The fact that the grid and the thread blocks are two-dimensional</li>
<li>
<p>The call to <code>cudaDeviceSynchronize()</code></p>
<p>:::c
int main(int argc, char **argv) {</p>
<p>float <em>hostA, </em>hostB, <em>hostC;<br>
  float </em>deviceA, <em>deviceB, </em>deviceC;<br>
  int numARows, numACols; // Rows, columns in the matrix A<br>
  int numBRows, numBCols; // Rows, columns in the matrix B<br>
  int numCRows, numCCols; // Rows, columns in the matrix C<br>
  int sizeA, sizeB, sizeC; // Size in memory of each of A, B and C<br>
  int gridXSize, gridYSize; // Number of thread blocks in x, y dimensions of grid<br>
  int blockSize; // Number of threads in block</p>
<p>// Allocate and populate the A and B matrices<br>
  // hostA and hostB, and get numARows, numACols,<br>
  // numBRows, numBCols</p>
<p>// Set numCRows and numCCols<br>
  numCRows = numARows;<br>
  numCCols = numBCols;<br>
<br>
  // Allocate the C matrix<br>
  hostC = (float<em>)malloc(numCRows</em>numCCols<em>sizeof(float));<br>
<br>
  // Allocate GPU memory<br>
  sizeA = numARows</em>numACols<em>sizeof(float);<br>
  sizeB = numBRows</em>numBCols<em>sizeof(float);<br>
  sizeC = numCRows</em>numCCols<em>sizeof(float);<br>
  cudaCheck(cudaMalloc((void </em><em>) &amp;deviceA, sizeA));<br>
  cudaCheck(cudaMalloc((void </em><em>) &amp;deviceB, sizeB));<br>
  cudaCheck(cudaMalloc((void </em>*) &amp;deviceC, sizeC));</p>
<p>// Copy data to the GPU<br>
  cudaCheck(cudaMemcpy(deviceA, hostA, sizeA,
  cudaMemcpyHostToDevice));<br>
  cudaCheck(cudaMemcpy(deviceB, hostB, sizeB,
  cudaMemcpyHostToDevice));</p>
<p>// Initialize the grid and block dimensions<br>
  blockSize = 16;<br>
  gridXSize = (numCCols-1)/blockSize + 1;<br>
  gridYSize = (numCRows-1)/blockSize + 1;<br>
  dim3 dimGrid(gridXSize, gridYSize, 1);<br>
  dim3 dimBlock(blockSize, blockSize, 1);<br>
<br>
  // Launch the GPU Kernel<br>
  matrixMultiply&lt;&lt;<dimGrid,dimBlock>&gt;&gt;(deviceA, deviceB,<br>
    deviceC, numACols,<br>
    numBRows, numBCols,<br>
    numCRows, numCCols);<br>
  cudaDeviceSynchronize();</p>
<p>// Copy the GPU memory back to the CPU<br>
  cudaCheck(cudaMemcpy(hostC, deviceC, sizeC,
  cudaMemcpyDeviceToHost));</p>
<p>// Free the GPU memory<br>
  cudaCheck(cudaFree(deviceA));<br>
  cudaCheck(cudaFree(deviceB));<br>
  cudaCheck(cudaFree(deviceC));<br>
<br>
  // Do something with the solution, free the host memory, return</p>
<p>}  </p>
</li>
</ol>
<p>The call to <code>cudaDeviceSynchronize()</code> ensures that all threads have
finished before the host code proceeds any further.</p>
<h3>Performance analysis of the naive implementation</h3>
<p>Clearly, each of the <span class="math">\(mp\)</span> elements of <span class="math">\(C\)</span> requires a full row of <span class="math">\(A\)</span> and
a full column of <span class="math">\(B\)</span> - both of length <span class="math">\(n\)</span> - to be read from memory, and
one value to be written back. Hence there are <span class="math">\((2n + 1)mp\)</span> memory
accesses. Re-examining the kernel, we see that there are two floating
point operations per iteration of the inner loop (one multiply and one
add), and <span class="math">\(n\)</span> iterations of that loop, which is completed for each of
the <span class="math">\(mp\)</span> elements in the product matrix. Hence, there are <span class="math">\(2nmp\)</span> FLOP,
and the CGMA is <span class="math">\(2n/(2n + 1)\)</span>; which is effectively 1, except when the
matrices are very small. With a memory bandwidth of 150 GB/s, the
algorithm is limited to just under 150/8 = 20 GFLOP/s (assuming double
precision), which is still less than 2% of the available compute of our
nominal 1 TFLOP GPU.</p>
<h2>Improving on the naive implementation</h2>
<p>However, it turns out that we can improve on this. So far, all the data
storage has been in global memory, because that's the only permissible
location for CUDA memory allocations in the host code, and that's where
the data stays unless we explicitly move it, once inside the kernel
function (we'll see how later). It's also clear that in this algorithm
data gets re-used frequently. Every row of matrix <span class="math">\(A\)</span> is used <span class="math">\(p\)</span> times
and every column of matrix <span class="math">\(B\)</span> is used <span class="math">\(m\)</span> times. If we contrive an
algorithm that gets the necessary data into shared memory before it is
needed, and keeps it there while it is being re-used, then we can
clearly reduce the global memory accesses.</p>
<p>However, it's not as though we can read <span class="math">\(A\)</span> and <span class="math">\(B\)</span> into shared memory
and have them accessible to all the threads working on the computation;
shared memory isn't globally accessible, despite the name, but is
instead local to a single streaming multiprocessor, and only 'shared'
amongst the threads in whichever thread block is currently assigned to
the SM. Hence our goal is to ensure that the threads in a given thread
block have the subset of input data they need available in their SM's
shared memory, under the general assumption that because of the small
size of the shared memory, not all of the needed data will fit in at
once.</p>
<p><img alt="tiled_matrix_multiplication_2" src="https://0x7df.github.io/images/tiled_matrix_multiplication_2.png?w=285"></p>
<p>Consider a thread block covering an area of the product matrix <span class="math">\(C\)</span>, which is <span class="math">\(a\)</span>
rows high by <span class="math">\(a\)</span> columns wide, with the top-left element being <span class="math">\(i\)</span>, <span class="math">\(j\)</span>
and the bottom-right therefore being <span class="math">\(i+a, j+a\)</span>. This is shown in the
figure. To compute these values, the rows <span class="math">\(i, i+1, ..., i+a\)</span> of matrix
<span class="math">\(A\)</span> and columns <span class="math">\(j, j+1, ..., j+a\)</span> of matrix <span class="math">\(B\)</span> are required,
comprising horizontal and vertical strips, respectively, of dimension <span class="math">\(a
\times n\)</span> elements. We assume in general these strips comprise too much data
to move all together to shared memory. Instead, we move a block of
elements from the strip of <span class="math">\(A\)</span>, and a block of elements from the strip
of <span class="math">\(B\)</span> - i.e. two blocks of size <span class="math">\(a \times a\)</span>, one from each matrix; we
will refer to these as <em>tiles</em>. Performing matrix multiplication on
these two tiles creates a tile of partial sums in the <span class="math">\(C\)</span> elements. When
the next pair of tiles from <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are retrieved, the partial sums
are further incremented, until eventually the full strips have been
processed and the final answers are available.</p>
<p>There is still some duplication of global memory accesses, because any
given strip of <span class="math">\(A\)</span> will be required by all the thread blocks of the <span class="math">\(C\)</span>
matrix that share the same row indices; and any given strip of <span class="math">\(B\)</span> will
be required by all the thread blocks of the <span class="math">\(C\)</span> matrix that share the
same column indices. However, we can see that there is at least <em>some</em>
re-use of data in shared memory; each sub-row of the tile from <span class="math">\(A\)</span> gets
re-used <span class="math">\(a\)</span> times (for the <span class="math">\(a\)</span> elements of the output matrix that have
the same row index), as does each sub-column of the tile from <span class="math">\(B\)</span>. This
data re-use reduces the retrievals from global memory by a factor of
<span class="math">\(a\)</span>.</p>
<p>Here is the kernel for tiled matrix multiplication.</p>
<div class="highlight"><pre><span></span><code><span class="n">__global__</span> <span class="kt">void</span> <span class="n">matrixMultiply</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">C</span><span class="p">,</span>  
  <span class="kt">int</span> <span class="n">numARows</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numACols</span><span class="p">,</span>  
  <span class="kt">int</span> <span class="n">numBRows</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numBCols</span><span class="p">,</span>  
  <span class="kt">int</span> <span class="n">numCRows</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numCCols</span><span class="p">)</span> <span class="p">{</span>

  <span class="c1">// Define device shared-memory storage for  </span>
  <span class="c1">// tiles of the matrices  </span>
  <span class="c1">// Scope: each tile is accessible by a single  </span>
  <span class="c1">// block of threads  </span>
  <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">tileA</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>  
  <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">tileB</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>

  <span class="c1">// Define abbreviated variables for the  </span>
  <span class="c1">// block and thread IDs  </span>
  <span class="c1">// Scope: stored in registers and therefore  </span>
  <span class="c1">// accessible by single threads  </span>
  <span class="kt">int</span> <span class="n">bx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>  
  <span class="kt">int</span> <span class="n">by</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>  
  <span class="kt">int</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>  
  <span class="kt">int</span> <span class="n">ty</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

  <span class="c1">// Each thread is responsible for a single  </span>
  <span class="c1">// element of the product matrix C.  </span>
  <span class="c1">// Determine which element, from the block  </span>
  <span class="c1">// and thread indices  </span>
  <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">by</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">ty</span><span class="p">;</span>  
  <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">bx</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">tx</span><span class="p">;</span>

  <span class="c1">// Initialise a temp variable for the solution  </span>
  <span class="c1">// for this matrix element  </span>
  <span class="c1">// Scope: in register, private to individual thread  </span>
  <span class="kt">float</span> <span class="n">Ctemp</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

  <span class="c1">// Loop over the tiles in the A and B matrices  </span>
  <span class="c1">// that will contribute to the calculation of  </span>
  <span class="c1">// this element in the product matrix. We are  </span>
  <span class="c1">// looping over columns of A for a given row  </span>
  <span class="c1">// (equal to the row index of the C element),  </span>
  <span class="c1">// and over rows of the B matrix for a given  </span>
  <span class="c1">// column index (equal to the column index of  </span>
  <span class="c1">// the C element)  </span>
  <span class="kt">int</span> <span class="n">numTiles</span> <span class="o">=</span> <span class="p">(</span><span class="n">numACols</span><span class="mi">-1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tl</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tl</span> <span class="o">&lt;</span> <span class="n">numTiles</span><span class="p">;</span> <span class="o">++</span><span class="n">tl</span><span class="p">)</span> <span class="p">{</span>

    <span class="c1">// Load the tiles into shared memory, so all  </span>
    <span class="c1">// threads in the block have access to the  </span>
    <span class="c1">// whole tiles. Each thread needs to load only  </span>
    <span class="c1">// a single value of each of the A and B tiles.  </span>
    <span class="k">if</span> <span class="p">((</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">numARows</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">tl</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">tx</span> <span class="o">&lt;</span> <span class="n">numACols</span><span class="p">))</span> <span class="p">{</span>  
      <span class="n">tileA</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">numACols</span> <span class="o">+</span> <span class="n">tl</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">tx</span><span class="p">];</span>  
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>  
      <span class="n">tileA</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">;</span>  
    <span class="p">}</span>  
    <span class="k">if</span> <span class="p">((</span><span class="n">tl</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">ty</span> <span class="o">&lt;</span> <span class="n">numBRows</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">numBCols</span><span class="p">))</span> <span class="p">{</span>  
      <span class="n">tileB</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[(</span><span class="n">tl</span><span class="o">*</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">ty</span><span class="p">)</span><span class="o">*</span><span class="n">numBCols</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>  
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>  
      <span class="n">tileB</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">;</span>  
    <span class="p">}</span>  
    <span class="n">__syncthreads</span><span class="p">();</span>

    <span class="c1">// Loop over the elements within the A and B  </span>
    <span class="c1">// tiles that contribute to this element of C  </span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>  
      <span class="n">Ctemp</span> <span class="o">+=</span> <span class="n">tileA</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">tileB</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">tx</span><span class="p">];</span>  
    <span class="p">}</span>  
    <span class="n">__syncthreads</span><span class="p">();</span>  
  <span class="p">}</span>

  <span class="c1">// Write the final value into the output array  </span>
  <span class="k">if</span> <span class="p">((</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">numARows</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">numBCols</span><span class="p">))</span> <span class="p">{</span>  
    <span class="n">C</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">numBCols</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">Ctemp</span><span class="p">;</span>  
  <span class="p">}</span>  
<span class="p">}</span>
</code></pre></div>

<p>In each thread block, the <span class="math">\(a^2\)</span> threads load two float values each and
perform <span class="math">\(2a\)</span> floating-point operations to compute the dot product of the
row and column sub-sections (both of length <span class="math">\(a\)</span>) required for the single
output matrix element it holds. Hence there are <span class="math">\(2a\)</span> computations for
two memory loads, which gives a CGMA ratio of <span class="math">\(a\)</span>. For the naive
implementation it was 1, so we have improved the CGMA by a factor of <span class="math">\(a\)</span>
by tiling the data.</p>
<p>There are a few other things to note in the kernel.</p>
<ol>
<li>The use of the <code>__shared__</code> identifier in the allocations statements
    for <code>tileA</code> and <code>tileB</code> (which are the temporary storage arrays for
    the tiles of <span class="math">\(A\)</span> and <span class="math">\(B\)</span>). This keyword is how we cause the storage
    to be allocated in shared memory (and therefore it can be used only
    in <code>__device__</code> functions, not <code>__host__</code> functions).</li>
<li><code>TILE_WIDTH</code> is a C macro that we assume has been defined elsewhere.</li>
<li>
<p>Calculation of the <span class="math">\(C\)</span> element indices <code>row</code> and <code>col</code> is done using
    <code>TILE_WIDTH</code>, where previously <code>blockDim.x</code> and <code>blockDim.y</code>
    appeared. This works because we have <em>defined</em> the tile to be the
    same size as the thread block. In theory it could be different, but
    doing so gives us the very convenient consequence that each thread
    needs only to load a single element from each of <span class="math">\(A\)</span> and <span class="math">\(B\)</span> into
    shared memory to construct the tiles. This means the host code that
    calls the kernel needs to use <code>TILE_WIDTH</code> to define the block size:</p>
<div class="highlight"><pre><span></span><code><span class="n">gridXSize</span> <span class="o">=</span> <span class="p">(</span><span class="n">numCCols</span><span class="mi">-1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>  
<span class="n">gridYSize</span> <span class="o">=</span> <span class="p">(</span><span class="n">numCRows</span><span class="mi">-1</span><span class="p">)</span><span class="o">/</span><span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>  
<span class="n">dim3</span> <span class="nf">DimGrid</span><span class="p">(</span><span class="n">gridXSize</span><span class="p">,</span> <span class="n">gridYSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// gridSize blocks in the</span>
<span class="n">grid</span>  
<span class="n">dim3</span> <span class="n">DimBlock</span><span class="p">(</span><span class="n">TILE_WIDTH</span><span class="p">,</span> <span class="n">TILE_WIDTH</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// blockSize threads in</span>
<span class="n">each</span> <span class="n">block</span>  
<span class="n">matrixMultiply</span><span class="o">&lt;&lt;&lt;</span><span class="n">DimGrid</span><span class="p">,</span><span class="n">DimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">deviceA</span><span class="p">,</span> <span class="n">deviceB</span><span class="p">,</span>
<span class="n">deviceC</span><span class="p">,</span> <span class="p">...</span>
</code></pre></div>

</li>
<li>
<p>We have put some logic around the statements that transfer data to
    the shared-memory tile storage. Since we can't guarantee that there
    will be a whole number of thread blocks in the matrix, this prevents
    threads whose <code>row</code>, <code>col</code> indices are outside the bounds of either
    <em>A</em> or <em>B</em> from attempting to retrieve data that isn't there.</p>
</li>
<li>The appearance of <code>__syncthreads()</code>. This is a barrier
    synchronization across all threads that ensures all threads complete
    any work up to this point before any proceed further. Without this,
    some threads could move on to begin computing matrix elements before
    other threads have loaded the correct data into shared memory, and
    out-of-date data could be used.</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


        <div>
            <h3 id="comments">COMMENTS</h3>
        </div>

        <div id="comment-post">
            <form method="POST" action="https://comment-bot-0x7df.herokuapp.com/v2/entry/0x7df/0x7df.github.io/master/comments">
                <input name="options[redirect]" type="hidden" value="https://0x7df.github.io/cuda-basics-part-2">
                <input name="fields[url]" type="hidden" value="cuda-basics-part-2">
                <div><label>Name: <input name="fields[name]" type="text"></label></div>
                <div><label>E-mail: <input name="fields[email]" type="email"></label></div>
                <div><label>Message: <textarea name="fields[message]"></textarea></label></div>
                <button type="submit">Submit</button>
            </form>
        </div>


    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="https://0x7df.github.io/archives.html">Archives</a></li>
                            <li><a href="https://0x7df.github.io/tags.html">Tags</a></li>
                            <li><a href="https://0x7df.github.io/authors.html">Authors</a></li>
                            <li><a href="https://0x7df.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">Atom Feed</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Social</div>
                        <ul class="list-unstyled">
                            <li><a href="https://twitter.com/@0x7df" target="_blank">Twitter</a></li>
                            <li><a href="https://github.com/0x7df" target="_blank">GitHub</a></li>
                            <li><a href="https://www.facebook.com/0x7df" target="_blank">Facebook</a></li>
                            <li><a href="https://stackoverflow.com/users/4802778" target="_blank">StackOverflow</a></li>
                            <li><a href="http://www.freecodecamp.com/0x7df" target="_blank">FreeCodeCamp</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Links</div>
                        <ul class="list-unstyled">
                            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
                            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
                            <li><a href="https://github.com/barrysteyn/pelican_plugin-render_math" target="_blank">Maths plugin</a></li>
                            <li><a href="https://github.com/molivier/nest" target="_blank">Nest theme</a></li>
                            <li><a href="" target="_blank"></a></li>
                            <li><a href="" target="_blank"></a></li>
                            <li><a href="" target="_blank"></a></li>
                        </ul>
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; 0x7df 2015</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>